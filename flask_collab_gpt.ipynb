{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flask_collab_gpt.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Lfg48RBVo4p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transformers\n",
        "!pip install newsapi-python\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gUf1KzQVbR5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from flask import Flask\n",
        "from flask_ngrok import run_with_ngrok\n",
        "from newsapi import NewsApiClient\n",
        "import json\n",
        "import datetime\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU6wHJZ0Wi2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sf8s550LZ4iR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def match_class(target):\n",
        "    def do_match(tag):\n",
        "        classes = tag.get('class', [])\n",
        "        return all(c in classes for c in target)\n",
        "    return do_match\n",
        "\n",
        "\n",
        "def get_important_info(articles):\n",
        "    important = {'articles': []}\n",
        "    for art in articles['articles']:\n",
        "        if art['title'] is None or art['description'] is None or \\\n",
        "                art['urlToImage'] is None or art['publishedAt'][:10] is None or \\\n",
        "                art['url'] is None:\n",
        "            continue\n",
        "        ness_art = {'title': art['title'],\n",
        "                    'description': art['description'],\n",
        "                    'image': art['urlToImage'],\n",
        "                    'date': art['publishedAt'][:10],\n",
        "                    'link': art['url']}\n",
        "        important['articles'].append(ness_art)\n",
        "    return important\n",
        "\n",
        "\n",
        "def get_prev_day():\n",
        "    return str(datetime.date.today()-datetime.timedelta(1))\n",
        "\n",
        "\n",
        "def generate_text(text, maxlen, model, tokenizer):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    indexed_tokens = tokenizer.encode(text, add_special_tokens=False, return_tensors='pt')\n",
        "    indexed_tokens = indexed_tokens.to(device)\n",
        "    output_sequences = model.generate(\n",
        "        input_ids=indexed_tokens,\n",
        "        max_length=maxlen,\n",
        "        temperature=1,\n",
        "        top_k=0,\n",
        "        top_p=0.9,\n",
        "        repetition_penalty=1.0,\n",
        "        do_sample=True,\n",
        "        num_return_sequences=1,\n",
        "    )\n",
        "    if len(output_sequences.shape) > 2:\n",
        "        output_sequences.squeeze_()\n",
        "    generated_sequences = []\n",
        "    for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
        "        generated_sequence = generated_sequence.tolist()\n",
        "        text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True)\n",
        "        total_sequence = (text + text[len(tokenizer.decode(indexed_tokens[0],\n",
        "                                                           clean_up_tokenization_spaces=True)):])\n",
        "        generated_sequences.append(total_sequence)\n",
        "    return clear(generated_sequences[0])\n",
        "\n",
        "\n",
        "def clear(string):\n",
        "    cleared = ''\n",
        "    for i in range(len(string)-1, 0, -1):\n",
        "        if string[i:] not in string[:i]:\n",
        "            cleared = string[:i+1]\n",
        "            break\n",
        "    return '.'.join(cleared.split('.')[:-1])+'.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aQUlS8UV0ju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)\n",
        "newsapi = NewsApiClient(api_key='ac0d27dde82341969f6645b174c34679')\n",
        "\n",
        "\n",
        "@app.route('/')\n",
        "def index():\n",
        "    return \"Hello, World!\"\n",
        "\n",
        "\n",
        "def get_top_articles():\n",
        "    all_articles = newsapi.get_top_headlines(language='en', page=1, page_size=20)\n",
        "    important = get_important_info(all_articles)\n",
        "    return json.dumps(important)\n",
        "\n",
        "\n",
        "@app.route('/getTags', methods=['GET'])\n",
        "def get_tags():\n",
        "    url = \"https://yandex.ru/news/export\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html')\n",
        "    classes = soup.find_all(match_class(['export__item']))\n",
        "    flag = False\n",
        "    # anton = [{'title': 'Популярное', 'tag': 'top'}]\n",
        "    anton = []\n",
        "    for class_ in classes:\n",
        "        try:\n",
        "            tag = class_.contents[0].attrs['href'].split('.rss')[0].split('/')[-1]\n",
        "            if tag == 'index':\n",
        "                if not flag:\n",
        "                    flag = True\n",
        "                else:\n",
        "                    break\n",
        "            anton.append({'title': class_.text, 'tag': tag})\n",
        "        except:\n",
        "            break\n",
        "\n",
        "    return json.dumps(anton, ensure_ascii=False)\n",
        "\n",
        "\n",
        "@app.route('/getNewsByTag/<string:newsTag>', methods=['GET'])\n",
        "def get_news_by_tag(newsTag):\n",
        "    if str(newsTag).lower() == 'top':\n",
        "        return get_top_articles()\n",
        "    all_articles = newsapi.get_everything(q=str(newsTag).lower(), language='en', sort_by='relevancy',\n",
        "                                          page=1, page_size=20, from_param=get_prev_day())\n",
        "    important = get_important_info(all_articles)\n",
        "    return json.dumps(important)\n",
        "\n",
        "\n",
        "@app.route('/genText/<string:prefix>', methods=['GET'])\n",
        "def gen_text(prefix):\n",
        "    return json.dumps(generate_text(prefix, np.random.randint(100, 200), model, tokenizer))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-XHtRwLbOzv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}